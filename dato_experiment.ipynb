{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data available from:\n",
    "http://groupware.les.inf.puc-rio.br/static/har/dataset-har-PUC-Rio-ugulino.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This trial license of GraphLab Create is assigned to birdmw@gmail.com and will expire on October 27, 2015. Please contact trial@dato.com for licensing options or to request a free non-commercial license for personal or academic use.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-3110 - Server binary: /home/birdmw/anaconda/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1445386703.log\n",
      "[INFO] GraphLab Server Version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.canvas.set_target('ipynb')\n",
    "gl.canvas.set_target('browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_data(data_frame):\n",
    "    \"\"\"\n",
    "    Accepts a data_frame object - converts UK units to US. Ascribes proper data types, and drops one bad data point\n",
    "    \"\"\"\n",
    "    data_frame = data_frame.drop(data_frame.index[[122076]]) #bad data point needs removal\n",
    "    for col in ['user', 'gender', 'class']:\n",
    "        data_frame[col] = data_frame[col].astype(str)\n",
    "    for col in ['age', 'weight', 'x1', 'x2', 'x3', 'x4', 'y1', 'y2', 'y3', 'y4', 'z1', 'z2', 'z3', 'z4']:\n",
    "        data_frame[col] = data_frame[col].astype(int)\n",
    "    for col in ['how_tall_in_meters', 'body_mass_index']:\n",
    "        data_frame[col] = data_frame[col].apply(lambda x: str(x).replace(\".\",\"\").replace(\",\",\".\"))\n",
    "        data_frame[col] = data_frame[col].astype(float)\n",
    "    return data_frame\n",
    "        \n",
    "def convert_columns_to_unique_ints(data_frame):\n",
    "    '''\n",
    "    Accepts a data_frame object\n",
    "    replaces column values with unique integers representing each unique value.\n",
    "    '''\n",
    "    for col_name in ['user', 'gender', 'class']:\n",
    "        original_values = data_frame[col_name].unique()\n",
    "        new_values = range(len(original_values))\n",
    "        print \"converting\", original_values\n",
    "        print \"into......\", new_values\n",
    "        data_frame[col_name] = data_frame[col_name].replace(original_values, new_values)\n",
    "    return data_frame\n",
    "\n",
    "def normalize(df, target):\n",
    "    \"\"\"\n",
    "    Accepts data frame object and a target column (in our case it is \"class\")\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        if feature_name != target:\n",
    "            max_value = df[feature_name].max()\n",
    "            min_value = df[feature_name].min()\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def pitch(x,y,z): #x,y,z = float, float, float\n",
    "    \"\"\"\n",
    "    Accepts accelerometer readings from a device and returns a pitch angle\n",
    "    \"\"\"\n",
    "    return math.atan( x / math.sqrt( y**2 + z**2 ) )\n",
    "\n",
    "def roll(x,y,z): #x,y,z = float, float, float\n",
    "    \"\"\"\n",
    "    Accepts accelerometer readings from a device and returns a roll angle\n",
    "    \"\"\"\n",
    "    return math.atan( y / math.sqrt( x**2 + z**2 ) )\n",
    "\n",
    "def add_pitch_roll(data_frame):\n",
    "    \"\"\"\n",
    "    Accepts a data frame and returns a dataframe with new columns for the calculated roll and pitch angles\"\"\"\n",
    "    result = data_frame.copy()\n",
    "    for i in range(1,5):\n",
    "        s = str(i)\n",
    "        result['pitch'+s] = np.vectorize(pitch)(result['x'+s], result['y'+s], result['z'+s])\n",
    "        result['roll'+s] = np.vectorize(roll)(result['x'+s], result['y'+s], result['z'+s])\n",
    "    return result\n",
    "\n",
    "def add_pitch_roll_diff(data_frame):\n",
    "    \"\"\"Accepts data frame\n",
    "       returns data frame with new columns:\n",
    "           difference between pitch/roll of sensors\"\"\"\n",
    "    result = data_frame.copy()\n",
    "    for i in range(1,5):\n",
    "        for j in range(1,5):\n",
    "            s1, s2 = str(i), str(j)\n",
    "            if s1 != s2:\n",
    "                result['pitchdiff'+s1+s2] = np.vectorize(np.subtract)(result['pitch'+s1], result['pitch'+s2])\n",
    "                result['rolldiff'+s1+s2] = np.vectorize(np.subtract)(result['roll'+s1], result['roll'+s2])\n",
    "    return result\n",
    "\n",
    "def PCAIT(data_frame, n_features):\n",
    "    \"\"\"Accepts data_frame\n",
    "    retuens data_frame\n",
    "    which as been PCA reduced down to n_features\"\"\"\n",
    "    result = data_frame.copy()\n",
    "    target = result.pop('class')\n",
    "    pca = PCA(n_components = n_features)\n",
    "    pca.fit(result)\n",
    "    result = pd.DataFrame(pca.transform(result))\n",
    "    result['class'] = target.astype(str)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = 'data/dataset-har-PUC-Rio-ugulino.csv'\n",
    "df = pd.read_csv (file_path, delimiter =\";\")\n",
    "df = fix_data(df) #call once per read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting ['debora' 'katia' 'wallace' 'jose_carlos']\n",
      "into...... [0, 1, 2, 3]\n",
      "converting ['Woman' 'Man']\n",
      "into...... [0, 1]\n",
      "converting ['sitting' 'sittingdown' 'standing' 'standingup' 'walking']\n",
      "into...... [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "df = convert_columns_to_unique_ints(df) \n",
    "df = add_pitch_roll(df)\n",
    "df = add_pitch_roll_diff(df)\n",
    "df = PCAIT(df, n_features=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165632\n",
      "165631\n",
      "0    50631\n",
      "2    47370\n",
      "4    43389\n",
      "3    12414\n",
      "1    11827\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        float64\n",
       "1        float64\n",
       "2        float64\n",
       "3        float64\n",
       "4        float64\n",
       "5        float64\n",
       "6        float64\n",
       "7        float64\n",
       "8        float64\n",
       "9        float64\n",
       "10       float64\n",
       "11       float64\n",
       "12       float64\n",
       "13       float64\n",
       "14       float64\n",
       "15       float64\n",
       "16       float64\n",
       "17       float64\n",
       "18       float64\n",
       "19       float64\n",
       "20       float64\n",
       "21       float64\n",
       "22       float64\n",
       "23       float64\n",
       "24       float64\n",
       "class     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(df)\n",
    "df = df.dropna()\n",
    "print len(df)\n",
    "print df['class'].value_counts()\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = normalize(df, 'class' )\n",
    "sf = gl.SFrame(data = df)   #turn data frame into GraphLab SFrame object\n",
    "df['class'] = df['class'].astype(float).fillna(0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#combining height with roll-pitch difference\\nfor i in range(1,5):\\n    for j in range(1,5):\\n        print i, j\\n        s1, s2 = str(i), str(j)\\n        if s1 != s2:\\n            quadratic = gl.feature_engineering.create(sf, gl.feature_engineering.QuadraticFeatures(features = ['pitchdiff'+s1+s2, 'how_tall_in_meters']))\\n            sf = quadratic.transform(sf)\\n            quadratic = gl.feature_engineering.create(sf, gl.feature_engineering.QuadraticFeatures(features = ['rolldiff'+s1+s2, 'how_tall_in_meters']))\\n            sf = quadratic.transform(sf)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#combining height with roll-pitch difference\n",
    "for i in range(1,5):\n",
    "    for j in range(1,5):\n",
    "        print i, j\n",
    "        s1, s2 = str(i), str(j)\n",
    "        if s1 != s2:\n",
    "            quadratic = gl.feature_engineering.create(sf, gl.feature_engineering.QuadraticFeatures(features = ['pitchdiff'+s1+s2, 'how_tall_in_meters']))\n",
    "            sf = quadratic.transform(sf)\n",
    "            quadratic = gl.feature_engineering.create(sf, gl.feature_engineering.QuadraticFeatures(features = ['rolldiff'+s1+s2, 'how_tall_in_meters']))\n",
    "            sf = quadratic.transform(sf)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sf.head()  # take a look at the new columns that have been added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILD A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Validating job.\n",
      "[INFO] Validation complete. Job: 'Model-Parameter-Search-Oct-21-2015-01-30-4000000' ready for execution\n",
      "[INFO] Job: 'Model-Parameter-Search-Oct-21-2015-01-30-4000000' scheduled.\n",
      "[INFO] Validating job.\n",
      "[INFO] A job with name 'Model-Parameter-Search-Oct-21-2015-01-30-4000000' already exists. Renaming the job to 'Model-Parameter-Search-Oct-21-2015-01-30-4000000-89c71'.\n",
      "[INFO] Validation complete. Job: 'Model-Parameter-Search-Oct-21-2015-01-30-4000000-89c71' ready for execution\n",
      "[INFO] Job: 'Model-Parameter-Search-Oct-21-2015-01-30-4000000-89c71' scheduled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">model_id</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">column_subsample</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">max_depth</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">max_iterations</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">min_child_weight</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">min_loss_reduction</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">row_subsample</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">step_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.55</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.85</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.65</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.75</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.65</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.75</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.65</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">170</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.75</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.55</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">170</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.75</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.55</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.85</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.55</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.75</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.65</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.85</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">7</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.65</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">180</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.75</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.55</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">170</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.85</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.3</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">target</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">training_accuracy</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">validation_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.995828510122</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.993184148622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.999837579798</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.996501598408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.999795228109</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.996501598408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.996984136558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.997104771096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.997104771096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.997104771096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.999858780998</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.997165088365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.997225405634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">class</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.997346040171</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 11 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tmodel_id\tint\n",
       "\tcolumn_subsample\tfloat\n",
       "\tmax_depth\tint\n",
       "\tmax_iterations\tint\n",
       "\tmin_child_weight\tint\n",
       "\tmin_loss_reduction\tint\n",
       "\trow_subsample\tfloat\n",
       "\tstep_size\tfloat\n",
       "\ttarget\tstr\n",
       "\ttraining_accuracy\tfloat\n",
       "\tvalidation_accuracy\tfloat\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+----------+------------------+-----------+----------------+------------------+\n",
       "| model_id | column_subsample | max_depth | max_iterations | min_child_weight |\n",
       "+----------+------------------+-----------+----------------+------------------+\n",
       "|    9     |       0.55       |     8     |      180       |        4         |\n",
       "|    5     |       0.65       |     9     |      180       |        4         |\n",
       "|    4     |       0.65       |     8     |      180       |        1         |\n",
       "|    6     |       0.65       |     9     |      170       |        8         |\n",
       "|    8     |       0.55       |     9     |      170       |        2         |\n",
       "|    1     |       0.55       |     8     |      180       |        4         |\n",
       "|    2     |       0.55       |     9     |      180       |        1         |\n",
       "|    0     |       0.65       |     8     |      180       |        2         |\n",
       "|    7     |       0.65       |     9     |      180       |        4         |\n",
       "|    3     |       0.55       |     9     |      170       |        8         |\n",
       "+----------+------------------+-----------+----------------+------------------+\n",
       "+--------------------+---------------+-----------+--------+-------------------+\n",
       "| min_loss_reduction | row_subsample | step_size | target | training_accuracy |\n",
       "+--------------------+---------------+-----------+--------+-------------------+\n",
       "|         10         |      0.85     |    0.3    | class  |   0.995828510122  |\n",
       "|         1          |      0.75     |    0.3    | class  |   0.999837579798  |\n",
       "|         1          |      0.75     |    0.3    | class  |   0.999795228109  |\n",
       "|         0          |      0.75     |    0.4    | class  |        1.0        |\n",
       "|         0          |      0.75     |    0.4    | class  |        1.0        |\n",
       "|         0          |      0.85     |    0.3    | class  |        1.0        |\n",
       "|         0          |      0.75     |    0.3    | class  |        1.0        |\n",
       "|         1          |      0.85     |    0.3    | class  |   0.999858780998  |\n",
       "|         0          |      0.75     |    0.3    | class  |        1.0        |\n",
       "|         0          |      0.85     |    0.3    | class  |        1.0        |\n",
       "+--------------------+---------------+-----------+--------+-------------------+\n",
       "+---------------------+\n",
       "| validation_accuracy |\n",
       "+---------------------+\n",
       "|    0.993184148622   |\n",
       "|    0.996501598408   |\n",
       "|    0.996501598408   |\n",
       "|    0.996984136558   |\n",
       "|    0.997104771096   |\n",
       "|    0.997104771096   |\n",
       "|    0.997104771096   |\n",
       "|    0.997165088365   |\n",
       "|    0.997225405634   |\n",
       "|    0.997346040171   |\n",
       "+---------------------+\n",
       "[10 rows x 11 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### THIS CODE WILL TAKE A LONG TIME (5-10 minutes on a fast computer) ###\n",
    "# This is a snippit of the tuning code I used to hone in on the best parameters for our model\n",
    "# The actual seach involved tweaking these parameters many times\n",
    "\n",
    "\n",
    "train, test = sf.random_split(.9)  # split data into a training and test set\n",
    "params = dict([('target', 'class'),('max_iterations', [180, 170]),('max_depth', [9,8]),('step_size', [.3,.4]),\n",
    "               ('column_subsample', [.65, .55]), ('row_subsample', [.85, .75])\n",
    "              ])\n",
    "job = gl.model_parameter_search.create((train, test), gl.boosted_trees_classifier.create, params)\n",
    "job.get_results().sort('validation_accuracy')\n",
    "\n",
    "\n",
    "# given parameters (see: params), cycle through them and find the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: Boosted trees classifier:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 141799\n",
      "PROGRESS: Number of classes           : 5\n",
      "PROGRESS: Number of feature columns   : 18\n",
      "PROGRESS: Number of unpacked features : 18\n",
      "PROGRESS: Starting Boosted Trees\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS:   Iter      Accuracy          Elapsed time\n",
      "PROGRESS:         (training) (validation)\n",
      "PROGRESS:      0   9.529e-01   9.509e-01        0.32s\n",
      "PROGRESS:      1   9.677e-01   9.638e-01        0.75s\n",
      "PROGRESS:      2   9.759e-01   9.738e-01        1.29s\n",
      "PROGRESS:      3   9.810e-01   9.791e-01        1.89s\n",
      "PROGRESS:      4   9.843e-01   9.813e-01        2.19s\n",
      "PROGRESS:      5   9.854e-01   9.829e-01        2.53s\n",
      "PROGRESS:      6   9.878e-01   9.843e-01        2.97s\n",
      "PROGRESS:      7   9.891e-01   9.857e-01        3.33s\n",
      "PROGRESS:      8   9.904e-01   9.876e-01        3.62s\n",
      "PROGRESS:      9   9.913e-01   9.889e-01        3.92s\n",
      "PROGRESS:     10   9.921e-01   9.899e-01        4.24s\n",
      "PROGRESS:     11   9.927e-01   9.907e-01        4.54s\n",
      "PROGRESS:     12   9.935e-01   9.918e-01        4.91s\n",
      "PROGRESS:     13   9.942e-01   9.923e-01        5.32s\n",
      "PROGRESS:     14   9.947e-01   9.933e-01        5.68s\n",
      "PROGRESS:     15   9.954e-01   9.940e-01        6.24s\n",
      "PROGRESS:     16   9.957e-01   9.943e-01        6.54s\n",
      "PROGRESS:     17   9.959e-01   9.940e-01        6.87s\n",
      "PROGRESS:     18   9.962e-01   9.945e-01        7.20s\n",
      "PROGRESS:     19   9.964e-01   9.952e-01        7.52s\n",
      "PROGRESS:     20   9.968e-01   9.954e-01        7.81s\n",
      "PROGRESS:     21   9.971e-01   9.962e-01        8.08s\n",
      "PROGRESS:     22   9.973e-01   9.960e-01        8.41s\n",
      "PROGRESS:     23   9.975e-01   9.960e-01        8.69s\n",
      "PROGRESS:     24   9.977e-01   9.960e-01        8.96s\n",
      "PROGRESS:     25   9.980e-01   9.962e-01        9.28s\n",
      "PROGRESS:     26   9.981e-01   9.962e-01        9.56s\n",
      "PROGRESS:     27   9.982e-01   9.960e-01        9.84s\n",
      "PROGRESS:     28   9.983e-01   9.960e-01       10.11s\n",
      "PROGRESS:     29   9.984e-01   9.963e-01       10.39s\n",
      "PROGRESS:     30   9.986e-01   9.966e-01       10.65s\n",
      "PROGRESS:     31   9.988e-01   9.964e-01       10.99s\n",
      "PROGRESS:     32   9.988e-01   9.966e-01       11.28s\n",
      "PROGRESS:     33   9.989e-01   9.966e-01       11.55s\n",
      "PROGRESS:     34   9.990e-01   9.967e-01       11.87s\n",
      "PROGRESS:     35   9.991e-01   9.969e-01       12.18s\n",
      "PROGRESS:     36   9.991e-01   9.969e-01       12.49s\n",
      "PROGRESS:     37   9.992e-01   9.969e-01       12.83s\n",
      "PROGRESS:     38   9.992e-01   9.969e-01       13.09s\n",
      "PROGRESS:     39   9.993e-01   9.969e-01       13.39s\n",
      "PROGRESS:     40   9.993e-01   9.969e-01       13.68s\n",
      "PROGRESS:     41   9.993e-01   9.969e-01       13.99s\n",
      "PROGRESS:     42   9.994e-01   9.969e-01       14.27s\n",
      "PROGRESS:     43   9.994e-01   9.970e-01       14.59s\n",
      "PROGRESS:     44   9.994e-01   9.971e-01       14.93s\n",
      "PROGRESS:     45   9.994e-01   9.970e-01       15.27s\n",
      "PROGRESS:     46   9.995e-01   9.970e-01       15.63s\n",
      "PROGRESS:     47   9.996e-01   9.971e-01       15.93s\n",
      "PROGRESS:     48   9.996e-01   9.971e-01       16.21s\n",
      "PROGRESS:     49   9.996e-01   9.971e-01       16.51s\n",
      "PROGRESS:     50   9.996e-01   9.971e-01       16.81s\n",
      "PROGRESS:     51   9.997e-01   9.971e-01       17.15s\n",
      "PROGRESS:     52   9.997e-01   9.971e-01       17.43s\n",
      "PROGRESS:     53   9.997e-01   9.970e-01       17.69s\n",
      "PROGRESS:     54   9.997e-01   9.970e-01       17.94s\n",
      "PROGRESS:     55   9.997e-01   9.970e-01       18.22s\n",
      "PROGRESS:     56   9.997e-01   9.970e-01       18.55s\n",
      "PROGRESS:     57   9.997e-01   9.970e-01       18.80s\n",
      "PROGRESS:     58   9.998e-01   9.971e-01       19.15s\n",
      "PROGRESS:     59   9.998e-01   9.971e-01       19.45s\n",
      "PROGRESS:     60   9.998e-01   9.970e-01       19.76s\n",
      "PROGRESS:     61   9.998e-01   9.970e-01       20.18s\n",
      "PROGRESS:     62   9.998e-01   9.970e-01       20.48s\n",
      "PROGRESS:     63   9.998e-01   9.973e-01       20.76s\n",
      "PROGRESS:     64   9.999e-01   9.973e-01       21.03s\n",
      "PROGRESS:     65   9.998e-01   9.971e-01       21.29s\n",
      "PROGRESS:     66   9.999e-01   9.973e-01       21.56s\n",
      "PROGRESS:     67   9.999e-01   9.971e-01       22.01s\n",
      "PROGRESS:     68   9.999e-01   9.973e-01       22.27s\n",
      "PROGRESS:     69   9.999e-01   9.973e-01       22.52s\n",
      "PROGRESS:     70   9.999e-01   9.973e-01       22.80s\n",
      "PROGRESS:     71   9.999e-01   9.973e-01       23.09s\n",
      "PROGRESS:     72   9.999e-01   9.973e-01       23.47s\n",
      "PROGRESS:     73   9.999e-01   9.973e-01       23.74s\n",
      "PROGRESS:     74   9.999e-01   9.973e-01       24.01s\n",
      "PROGRESS:     75   9.999e-01   9.971e-01       24.29s\n",
      "PROGRESS:     76   9.999e-01   9.971e-01       24.60s\n",
      "PROGRESS:     77   9.999e-01   9.971e-01       24.92s\n",
      "PROGRESS:     78   9.999e-01   9.971e-01       25.23s\n",
      "PROGRESS:     79   9.999e-01   9.971e-01       25.51s\n",
      "PROGRESS:     80   9.999e-01   9.971e-01       25.79s\n",
      "PROGRESS:     81   9.999e-01   9.971e-01       26.07s\n",
      "PROGRESS:     82   9.999e-01   9.973e-01       26.33s\n",
      "PROGRESS:     83   9.999e-01   9.973e-01       26.59s\n",
      "PROGRESS:     84   9.999e-01   9.973e-01       26.83s\n",
      "PROGRESS:     85   9.999e-01   9.973e-01       27.06s\n",
      "PROGRESS:     86   9.999e-01   9.971e-01       27.31s\n",
      "PROGRESS:     87   9.999e-01   9.973e-01       27.59s\n",
      "PROGRESS:     88   9.999e-01   9.973e-01       27.83s\n",
      "PROGRESS:     89   9.999e-01   9.971e-01       28.06s\n",
      "PROGRESS:     90   9.999e-01   9.973e-01       28.30s\n",
      "PROGRESS:     91   9.999e-01   9.973e-01       28.59s\n",
      "PROGRESS:     92   9.999e-01   9.973e-01       28.85s\n",
      "PROGRESS:     93   9.999e-01   9.973e-01       29.10s\n",
      "PROGRESS:     94   9.999e-01   9.971e-01       29.36s\n",
      "PROGRESS:     95   9.999e-01   9.971e-01       29.61s\n",
      "PROGRESS:     96   9.999e-01   9.971e-01       29.88s\n",
      "PROGRESS:     97   1.000e+00   9.973e-01       30.14s\n",
      "PROGRESS:     98   1.000e+00   9.973e-01       30.40s\n",
      "PROGRESS:     99   1.000e+00   9.973e-01       30.64s\n",
      "PROGRESS:    100   1.000e+00   9.973e-01       30.86s\n",
      "PROGRESS:    101   1.000e+00   9.974e-01       31.15s\n",
      "PROGRESS:    102   1.000e+00   9.973e-01       31.42s\n",
      "PROGRESS:    103   1.000e+00   9.973e-01       31.70s\n",
      "PROGRESS:    104   1.000e+00   9.973e-01       31.96s\n",
      "PROGRESS:    105   1.000e+00   9.973e-01       32.24s\n",
      "PROGRESS:    106   1.000e+00   9.973e-01       32.52s\n",
      "PROGRESS:    107   1.000e+00   9.973e-01       33.07s\n",
      "PROGRESS:    108   1.000e+00   9.971e-01       33.36s\n",
      "PROGRESS:    109   1.000e+00   9.971e-01       33.63s\n",
      "PROGRESS:    110   1.000e+00   9.973e-01       33.99s\n",
      "PROGRESS:    111   1.000e+00   9.973e-01       34.23s\n",
      "PROGRESS:    112   1.000e+00   9.973e-01       34.47s\n",
      "PROGRESS:    113   1.000e+00   9.973e-01       34.71s\n",
      "PROGRESS:    114   1.000e+00   9.973e-01       34.94s\n",
      "PROGRESS:    115   1.000e+00   9.973e-01       35.27s\n",
      "PROGRESS:    116   1.000e+00   9.973e-01       35.57s\n",
      "PROGRESS:    117   1.000e+00   9.973e-01       35.85s\n",
      "PROGRESS:    118   1.000e+00   9.973e-01       36.12s\n",
      "PROGRESS:    119   1.000e+00   9.973e-01       36.37s\n",
      "PROGRESS:    120   1.000e+00   9.973e-01       36.67s\n",
      "PROGRESS:    121   1.000e+00   9.973e-01       36.93s\n",
      "PROGRESS:    122   1.000e+00   9.973e-01       37.20s\n",
      "PROGRESS:    123   1.000e+00   9.973e-01       37.46s\n",
      "PROGRESS:    124   1.000e+00   9.973e-01       37.72s\n",
      "PROGRESS:    125   1.000e+00   9.973e-01       37.99s\n",
      "PROGRESS:    126   1.000e+00   9.973e-01       38.23s\n",
      "PROGRESS:    127   1.000e+00   9.973e-01       38.50s\n",
      "PROGRESS:    128   1.000e+00   9.973e-01       38.77s\n",
      "PROGRESS:    129   1.000e+00   9.973e-01       39.01s\n",
      "PROGRESS:    130   1.000e+00   9.973e-01       39.26s\n",
      "PROGRESS:    131   1.000e+00   9.973e-01       39.51s\n",
      "PROGRESS:    132   1.000e+00   9.974e-01       39.76s\n",
      "PROGRESS:    133   1.000e+00   9.974e-01       40.00s\n",
      "PROGRESS:    134   1.000e+00   9.974e-01       40.22s\n",
      "PROGRESS:    135   1.000e+00   9.974e-01       40.44s\n",
      "PROGRESS:    136   1.000e+00   9.974e-01       40.69s\n",
      "PROGRESS:    137   1.000e+00   9.974e-01       40.92s\n",
      "PROGRESS:    138   1.000e+00   9.974e-01       41.19s\n",
      "PROGRESS:    139   1.000e+00   9.974e-01       41.50s\n",
      "PROGRESS:    140   1.000e+00   9.974e-01       41.86s\n",
      "PROGRESS:    141   1.000e+00   9.974e-01       42.17s\n",
      "PROGRESS:    142   1.000e+00   9.974e-01       42.42s\n",
      "PROGRESS:    143   1.000e+00   9.974e-01       42.68s\n",
      "PROGRESS:    144   1.000e+00   9.974e-01       42.95s\n",
      "PROGRESS:    145   1.000e+00   9.974e-01       43.23s\n",
      "PROGRESS:    146   1.000e+00   9.974e-01       43.44s\n",
      "PROGRESS:    147   1.000e+00   9.974e-01       43.69s\n",
      "PROGRESS:    148   1.000e+00   9.974e-01       43.95s\n",
      "PROGRESS:    149   1.000e+00   9.974e-01       44.22s\n",
      "PROGRESS:    150   1.000e+00   9.974e-01       44.50s\n",
      "PROGRESS:    151   1.000e+00   9.974e-01       44.74s\n",
      "PROGRESS:    152   1.000e+00   9.974e-01       44.97s\n",
      "PROGRESS:    153   1.000e+00   9.974e-01       45.30s\n",
      "PROGRESS:    154   1.000e+00   9.974e-01       45.54s\n",
      "PROGRESS:    155   1.000e+00   9.974e-01       45.80s\n",
      "PROGRESS:    156   1.000e+00   9.974e-01       46.16s\n",
      "PROGRESS:    157   1.000e+00   9.974e-01       46.39s\n",
      "PROGRESS:    158   1.000e+00   9.974e-01       46.64s\n",
      "PROGRESS:    159   1.000e+00   9.974e-01       46.87s\n",
      "PROGRESS:    160   1.000e+00   9.974e-01       47.08s\n",
      "PROGRESS:    161   1.000e+00   9.974e-01       47.30s\n",
      "PROGRESS:    162   1.000e+00   9.974e-01       47.55s\n",
      "PROGRESS:    163   1.000e+00   9.974e-01       47.80s\n",
      "PROGRESS:    164   1.000e+00   9.974e-01       48.02s\n",
      "PROGRESS:    165   1.000e+00   9.974e-01       48.26s\n",
      "PROGRESS:    166   1.000e+00   9.974e-01       48.46s\n",
      "PROGRESS:    167   1.000e+00   9.974e-01       48.70s\n",
      "PROGRESS:    168   1.000e+00   9.974e-01       48.93s\n",
      "PROGRESS:    169   1.000e+00   9.974e-01       49.20s\n",
      "{'confusion_matrix': Columns:\n",
      "\ttarget_label\tint\n",
      "\tpredicted_label\tint\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 15\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|      3       |        4        |   7   |\n",
      "|      3       |        3        |  1239 |\n",
      "|      1       |        2        |   2   |\n",
      "|      2       |        4        |   11  |\n",
      "|      3       |        2        |   3   |\n",
      "|      4       |        3        |   5   |\n",
      "|      4       |        2        |   3   |\n",
      "|      0       |        0        |  5149 |\n",
      "|      4       |        1        |   1   |\n",
      "|      2       |        2        |  4766 |\n",
      "+--------------+-----------------+-------+\n",
      "[15 rows x 3 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'accuracy': 0.9972146533454436}\n"
     ]
    }
   ],
   "source": [
    "# take the best coefficients from the best row in the previous cell and build a model from it\n",
    "\n",
    "train, test = sf.random_split(.9)  #randomly choose 10% of the data and set aside for model verification\n",
    "model = gl.boosted_trees_classifier.create(train, target='class', column_subsample=.55, max_depth=9, \n",
    "                                           max_iterations=170, min_child_weight=8, min_loss_reduction=0,\n",
    "                                           row_subsample=.85, step_size=.3)\n",
    "# inspect the quality of the model\n",
    "res = model.evaluate(test)\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix': Columns:\n",
      "\ttarget_label\tint\n",
      "\tpredicted_label\tint\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 15\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|      3       |        4        |   7   |\n",
      "|      3       |        3        |  1239 |\n",
      "|      1       |        2        |   2   |\n",
      "|      2       |        4        |   11  |\n",
      "|      3       |        2        |   3   |\n",
      "|      4       |        3        |   5   |\n",
      "|      4       |        2        |   3   |\n",
      "|      0       |        0        |  5149 |\n",
      "|      4       |        1        |   1   |\n",
      "|      2       |        2        |  4766 |\n",
      "+--------------+-----------------+-------+\n",
      "[15 rows x 3 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'accuracy': 0.9972146533454436}\n"
     ]
    }
   ],
   "source": [
    "# inspect the quality of the model\n",
    "res = model.evaluate(test)\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the high accuracy of 99.72% beats out the best model of 99.6% seen on page 9 of this presentation:\n",
    "\n",
    "http://groupware.les.inf.puc-rio.br/public/2012.SBIA.Ugulino.WearableComputing-Presentation.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# record what features are important\n",
    "i_feats = model.get_feature_importance()\n",
    "f_imps = i_feats['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1d31aa6490>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d32794890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that of the 50 features, some have high utility (2600 on the y-axis) and some have low utility (almost 0), but all of them are useful.\n",
    "plt.plot(f_imps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|      feature       | count |\n",
      "+--------------------+-------+\n",
      "|         y2         |  2175 |\n",
      "|         z1         |  2172 |\n",
      "|         z2         |  2171 |\n",
      "|         x4         |  2030 |\n",
      "|         x3         |  1979 |\n",
      "|         x2         |  1921 |\n",
      "|         y3         |  1903 |\n",
      "|         z3         |  1887 |\n",
      "|         y4         |  1753 |\n",
      "|         z4         |  1601 |\n",
      "|         y1         |  1543 |\n",
      "|         x1         |  1437 |\n",
      "|        age         |  400  |\n",
      "|        user        |  315  |\n",
      "|  body_mass_index   |  310  |\n",
      "| how_tall_in_meters |  302  |\n",
      "|       weight       |  250  |\n",
      "|       gender       |  139  |\n",
      "+--------------------+-------+\n",
      "[18 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect that of the top features we see \n",
    "i_feats.print_rows(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROLL and PITCH compose many of the top features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We can say that feature engineering likely improved our model but to make sure we can run our model again without the feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting ['debora' 'katia' 'wallace' 'jose_carlos']\n",
      "into...... [0, 1, 2, 3]\n",
      "converting ['Woman' 'Man']\n",
      "into...... [0, 1]\n",
      "converting ['sitting' 'sittingdown' 'standing' 'standingup' 'walking']\n",
      "into...... [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv (file_path, delimiter =\";\")  #re-read in our data\n",
    "df = fix_data(df)  # fix our data\n",
    "df = convert_columns_to_unique_ints(df) # fix our data\n",
    "sf = gl.SFrame(data = df)   #turn data frame into GraphLab SFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRESS: Creating a validation set from 5 percent of training data. This may take a while.\n",
      "          You can set ``validation_set=None`` to disable validation tracking.\n",
      "\n",
      "PROGRESS: Boosted trees classifier:\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS: Number of examples          : 141304\n",
      "PROGRESS: Number of classes           : 5\n",
      "PROGRESS: Number of feature columns   : 18\n",
      "PROGRESS: Number of unpacked features : 18\n",
      "PROGRESS: Starting Boosted Trees\n",
      "PROGRESS: --------------------------------------------------------\n",
      "PROGRESS:   Iter      Accuracy          Elapsed time\n",
      "PROGRESS:         (training) (validation)\n",
      "PROGRESS:      0   9.628e-01   9.556e-01        0.39s\n",
      "PROGRESS:      1   9.751e-01   9.685e-01        0.71s\n",
      "PROGRESS:      2   9.788e-01   9.745e-01        1.04s\n",
      "PROGRESS:      3   9.813e-01   9.781e-01        1.37s\n",
      "PROGRESS:      4   9.821e-01   9.803e-01        1.70s\n",
      "PROGRESS:      5   9.836e-01   9.821e-01        2.05s\n",
      "PROGRESS:      6   9.842e-01   9.825e-01        2.35s\n",
      "PROGRESS:      7   9.847e-01   9.832e-01        2.65s\n",
      "PROGRESS:      8   9.848e-01   9.828e-01        2.98s\n",
      "PROGRESS:      9   9.854e-01   9.834e-01        3.29s\n",
      "PROGRESS:     10   9.859e-01   9.836e-01        3.60s\n",
      "PROGRESS:     11   9.864e-01   9.841e-01        3.90s\n",
      "PROGRESS:     12   9.864e-01   9.841e-01        4.23s\n",
      "PROGRESS:     13   9.867e-01   9.846e-01        4.53s\n",
      "PROGRESS:     14   9.874e-01   9.845e-01        4.85s\n",
      "PROGRESS:     15   9.877e-01   9.850e-01        5.15s\n",
      "PROGRESS:     16   9.883e-01   9.851e-01        5.59s\n",
      "PROGRESS:     17   9.888e-01   9.860e-01        5.88s\n",
      "PROGRESS:     18   9.891e-01   9.869e-01        6.18s\n",
      "PROGRESS:     19   9.895e-01   9.876e-01        6.50s\n",
      "PROGRESS:     20   9.900e-01   9.880e-01        6.82s\n",
      "PROGRESS:     21   9.904e-01   9.877e-01        7.13s\n",
      "PROGRESS:     22   9.908e-01   9.880e-01        7.45s\n",
      "PROGRESS:     23   9.912e-01   9.883e-01        7.77s\n",
      "PROGRESS:     24   9.916e-01   9.890e-01        8.10s\n",
      "PROGRESS:     25   9.920e-01   9.893e-01        8.43s\n",
      "PROGRESS:     26   9.923e-01   9.898e-01        8.76s\n",
      "PROGRESS:     27   9.927e-01   9.900e-01        9.07s\n",
      "PROGRESS:     28   9.930e-01   9.899e-01        9.36s\n",
      "PROGRESS:     29   9.932e-01   9.903e-01        9.66s\n",
      "PROGRESS:     30   9.934e-01   9.900e-01        9.96s\n",
      "PROGRESS:     31   9.937e-01   9.900e-01       10.25s\n",
      "PROGRESS:     32   9.938e-01   9.904e-01       10.55s\n",
      "PROGRESS:     33   9.940e-01   9.906e-01       10.84s\n",
      "PROGRESS:     34   9.942e-01   9.906e-01       11.18s\n",
      "PROGRESS:     35   9.944e-01   9.909e-01       11.49s\n",
      "PROGRESS:     36   9.947e-01   9.912e-01       11.79s\n",
      "PROGRESS:     37   9.949e-01   9.912e-01       12.10s\n",
      "PROGRESS:     38   9.952e-01   9.913e-01       12.40s\n",
      "PROGRESS:     39   9.954e-01   9.915e-01       12.73s\n",
      "PROGRESS:     40   9.956e-01   9.916e-01       13.05s\n",
      "PROGRESS:     41   9.957e-01   9.917e-01       13.36s\n",
      "PROGRESS:     42   9.959e-01   9.918e-01       13.68s\n",
      "PROGRESS:     43   9.961e-01   9.920e-01       14.04s\n",
      "PROGRESS:     44   9.962e-01   9.921e-01       14.36s\n",
      "PROGRESS:     45   9.964e-01   9.922e-01       14.68s\n",
      "PROGRESS:     46   9.966e-01   9.925e-01       15.01s\n",
      "PROGRESS:     47   9.968e-01   9.925e-01       15.32s\n",
      "PROGRESS:     48   9.969e-01   9.929e-01       15.63s\n",
      "PROGRESS:     49   9.970e-01   9.930e-01       15.93s\n",
      "PROGRESS:     50   9.971e-01   9.934e-01       16.24s\n",
      "PROGRESS:     51   9.973e-01   9.934e-01       16.56s\n",
      "PROGRESS:     52   9.974e-01   9.937e-01       16.87s\n",
      "PROGRESS:     53   9.975e-01   9.939e-01       17.22s\n",
      "PROGRESS:     54   9.976e-01   9.940e-01       17.52s\n",
      "PROGRESS:     55   9.977e-01   9.942e-01       17.88s\n",
      "PROGRESS:     56   9.978e-01   9.942e-01       18.17s\n",
      "PROGRESS:     57   9.979e-01   9.944e-01       18.48s\n",
      "PROGRESS:     58   9.980e-01   9.946e-01       18.83s\n",
      "PROGRESS:     59   9.981e-01   9.946e-01       19.13s\n",
      "PROGRESS:     60   9.981e-01   9.948e-01       19.43s\n",
      "PROGRESS:     61   9.982e-01   9.947e-01       19.72s\n",
      "PROGRESS:     62   9.983e-01   9.948e-01       20.02s\n",
      "PROGRESS:     63   9.983e-01   9.948e-01       20.33s\n",
      "PROGRESS:     64   9.984e-01   9.948e-01       20.63s\n",
      "PROGRESS:     65   9.984e-01   9.950e-01       20.93s\n",
      "PROGRESS:     66   9.985e-01   9.952e-01       21.22s\n",
      "PROGRESS:     67   9.985e-01   9.955e-01       21.53s\n",
      "PROGRESS:     68   9.985e-01   9.956e-01       21.85s\n",
      "PROGRESS:     69   9.986e-01   9.956e-01       22.12s\n",
      "PROGRESS:     70   9.986e-01   9.956e-01       22.40s\n",
      "PROGRESS:     71   9.986e-01   9.956e-01       22.68s\n",
      "PROGRESS:     72   9.987e-01   9.956e-01       22.96s\n",
      "PROGRESS:     73   9.987e-01   9.957e-01       23.30s\n",
      "PROGRESS:     74   9.988e-01   9.957e-01       23.60s\n",
      "PROGRESS:     75   9.988e-01   9.957e-01       23.89s\n",
      "PROGRESS:     76   9.989e-01   9.959e-01       24.20s\n",
      "PROGRESS:     77   9.990e-01   9.959e-01       24.48s\n",
      "PROGRESS:     78   9.990e-01   9.960e-01       24.76s\n",
      "PROGRESS:     79   9.991e-01   9.959e-01       25.02s\n",
      "PROGRESS:     80   9.992e-01   9.960e-01       25.30s\n",
      "PROGRESS:     81   9.992e-01   9.960e-01       25.61s\n",
      "PROGRESS:     82   9.992e-01   9.961e-01       25.90s\n",
      "PROGRESS:     83   9.993e-01   9.961e-01       26.21s\n",
      "PROGRESS:     84   9.993e-01   9.961e-01       26.52s\n",
      "PROGRESS:     85   9.993e-01   9.961e-01       26.83s\n",
      "PROGRESS:     86   9.994e-01   9.961e-01       27.12s\n",
      "PROGRESS:     87   9.994e-01   9.960e-01       27.41s\n",
      "PROGRESS:     88   9.994e-01   9.961e-01       27.70s\n",
      "PROGRESS:     89   9.994e-01   9.961e-01       28.02s\n",
      "PROGRESS:     90   9.994e-01   9.961e-01       28.31s\n",
      "PROGRESS:     91   9.995e-01   9.962e-01       28.58s\n",
      "PROGRESS:     92   9.995e-01   9.962e-01       28.86s\n",
      "PROGRESS:     93   9.995e-01   9.961e-01       29.13s\n",
      "PROGRESS:     94   9.995e-01   9.961e-01       29.46s\n",
      "PROGRESS:     95   9.996e-01   9.961e-01       29.75s\n",
      "PROGRESS:     96   9.996e-01   9.961e-01       30.05s\n",
      "PROGRESS:     97   9.996e-01   9.962e-01       30.35s\n",
      "PROGRESS:     98   9.996e-01   9.962e-01       30.63s\n",
      "PROGRESS:     99   9.996e-01   9.964e-01       30.97s\n",
      "PROGRESS:    100   9.996e-01   9.964e-01       31.25s\n",
      "PROGRESS:    101   9.997e-01   9.964e-01       31.55s\n",
      "PROGRESS:    102   9.997e-01   9.964e-01       31.85s\n",
      "PROGRESS:    103   9.997e-01   9.964e-01       32.14s\n",
      "PROGRESS:    104   9.997e-01   9.964e-01       32.47s\n",
      "PROGRESS:    105   9.997e-01   9.964e-01       32.76s\n",
      "PROGRESS:    106   9.997e-01   9.964e-01       33.05s\n",
      "PROGRESS:    107   9.997e-01   9.964e-01       33.33s\n",
      "PROGRESS:    108   9.998e-01   9.964e-01       33.63s\n",
      "PROGRESS:    109   9.998e-01   9.964e-01       33.94s\n",
      "PROGRESS:    110   9.998e-01   9.964e-01       34.25s\n",
      "PROGRESS:    111   9.998e-01   9.964e-01       34.53s\n",
      "PROGRESS:    112   9.998e-01   9.964e-01       34.80s\n",
      "PROGRESS:    113   9.998e-01   9.964e-01       35.08s\n",
      "PROGRESS:    114   9.998e-01   9.964e-01       35.35s\n",
      "PROGRESS:    115   9.998e-01   9.964e-01       35.65s\n",
      "PROGRESS:    116   9.998e-01   9.964e-01       35.95s\n",
      "PROGRESS:    117   9.998e-01   9.964e-01       36.24s\n",
      "PROGRESS:    118   9.998e-01   9.964e-01       36.53s\n",
      "PROGRESS:    119   9.998e-01   9.964e-01       36.82s\n",
      "PROGRESS:    120   9.998e-01   9.964e-01       37.14s\n",
      "PROGRESS:    121   9.998e-01   9.964e-01       37.41s\n",
      "PROGRESS:    122   9.999e-01   9.962e-01       37.68s\n",
      "PROGRESS:    123   9.999e-01   9.964e-01       37.96s\n",
      "PROGRESS:    124   9.999e-01   9.965e-01       38.24s\n",
      "PROGRESS:    125   9.999e-01   9.964e-01       38.53s\n",
      "PROGRESS:    126   9.999e-01   9.965e-01       38.80s\n",
      "PROGRESS:    127   9.999e-01   9.966e-01       39.09s\n",
      "PROGRESS:    128   9.999e-01   9.968e-01       39.37s\n",
      "PROGRESS:    129   9.999e-01   9.968e-01       39.66s\n",
      "PROGRESS:    130   9.999e-01   9.968e-01       39.95s\n",
      "PROGRESS:    131   9.999e-01   9.968e-01       40.25s\n",
      "PROGRESS:    132   9.999e-01   9.966e-01       40.53s\n",
      "PROGRESS:    133   9.999e-01   9.966e-01       40.81s\n",
      "PROGRESS:    134   1.000e+00   9.968e-01       41.10s\n",
      "PROGRESS:    135   1.000e+00   9.968e-01       41.39s\n",
      "PROGRESS:    136   1.000e+00   9.968e-01       41.70s\n",
      "PROGRESS:    137   1.000e+00   9.968e-01       41.99s\n",
      "PROGRESS:    138   1.000e+00   9.968e-01       42.29s\n",
      "PROGRESS:    139   1.000e+00   9.969e-01       42.58s\n",
      "PROGRESS:    140   1.000e+00   9.969e-01       42.88s\n",
      "PROGRESS:    141   1.000e+00   9.969e-01       43.20s\n",
      "PROGRESS:    142   1.000e+00   9.969e-01       43.49s\n",
      "PROGRESS:    143   1.000e+00   9.969e-01       43.77s\n",
      "PROGRESS:    144   1.000e+00   9.969e-01       44.05s\n",
      "PROGRESS:    145   1.000e+00   9.969e-01       44.32s\n",
      "PROGRESS:    146   1.000e+00   9.969e-01       44.61s\n",
      "PROGRESS:    147   1.000e+00   9.968e-01       44.92s\n",
      "PROGRESS:    148   1.000e+00   9.969e-01       45.21s\n",
      "PROGRESS:    149   1.000e+00   9.966e-01       45.54s\n",
      "PROGRESS:    150   1.000e+00   9.966e-01       45.83s\n",
      "PROGRESS:    151   1.000e+00   9.965e-01       46.14s\n",
      "PROGRESS:    152   1.000e+00   9.965e-01       46.44s\n",
      "PROGRESS:    153   1.000e+00   9.966e-01       46.72s\n",
      "PROGRESS:    154   1.000e+00   9.965e-01       47.02s\n",
      "PROGRESS:    155   1.000e+00   9.965e-01       47.29s\n",
      "PROGRESS:    156   1.000e+00   9.964e-01       47.58s\n",
      "PROGRESS:    157   1.000e+00   9.966e-01       47.88s\n",
      "PROGRESS:    158   1.000e+00   9.966e-01       48.16s\n",
      "PROGRESS:    159   1.000e+00   9.965e-01       48.42s\n",
      "PROGRESS:    160   1.000e+00   9.964e-01       48.70s\n",
      "PROGRESS:    161   1.000e+00   9.964e-01       48.97s\n",
      "PROGRESS:    162   1.000e+00   9.964e-01       49.23s\n",
      "PROGRESS:    163   1.000e+00   9.965e-01       49.50s\n",
      "PROGRESS:    164   1.000e+00   9.964e-01       49.77s\n",
      "PROGRESS:    165   1.000e+00   9.965e-01       50.05s\n",
      "PROGRESS:    166   1.000e+00   9.966e-01       50.31s\n",
      "PROGRESS:    167   1.000e+00   9.964e-01       50.58s\n",
      "PROGRESS:    168   1.000e+00   9.965e-01       50.90s\n",
      "PROGRESS:    169   1.000e+00   9.965e-01       51.19s\n",
      "PROGRESS:    170   1.000e+00   9.965e-01       51.50s\n",
      "PROGRESS:    171   1.000e+00   9.964e-01       51.80s\n",
      "PROGRESS:    172   1.000e+00   9.965e-01       52.09s\n",
      "PROGRESS:    173   1.000e+00   9.964e-01       52.39s\n",
      "PROGRESS:    174   1.000e+00   9.965e-01       52.65s\n",
      "PROGRESS:    175   1.000e+00   9.965e-01       52.91s\n",
      "PROGRESS:    176   1.000e+00   9.965e-01       53.18s\n",
      "PROGRESS:    177   1.000e+00   9.965e-01       53.50s\n",
      "PROGRESS:    178   1.000e+00   9.965e-01       53.80s\n",
      "PROGRESS:    179   1.000e+00   9.966e-01       54.09s\n"
     ]
    }
   ],
   "source": [
    "train, test = sf.random_split(.9)  #randomly choose 10% of the data and set aside for model verification\n",
    "model = gl.boosted_trees_classifier.create(train, target='class', column_subsample=.7, max_depth=9, \n",
    "                                           max_iterations=180, min_child_weight=1, min_loss_reduction=0,\n",
    "                                           row_subsample=.7, step_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix': Columns:\n",
      "\ttarget_label\tint\n",
      "\tpredicted_label\tint\n",
      "\tcount\tint\n",
      "\n",
      "Rows: 16\n",
      "\n",
      "Data:\n",
      "+--------------+-----------------+-------+\n",
      "| target_label | predicted_label | count |\n",
      "+--------------+-----------------+-------+\n",
      "|      3       |        4        |   3   |\n",
      "|      3       |        3        |  1252 |\n",
      "|      1       |        2        |   2   |\n",
      "|      2       |        4        |   3   |\n",
      "|      3       |        2        |   5   |\n",
      "|      1       |        3        |   7   |\n",
      "|      4       |        3        |   2   |\n",
      "|      4       |        2        |   7   |\n",
      "|      0       |        0        |  5154 |\n",
      "|      3       |        1        |   6   |\n",
      "+--------------+-----------------+-------+\n",
      "[16 rows x 3 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns., 'accuracy': 0.9973498765283383}\n"
     ]
    }
   ],
   "source": [
    "# inspect the quality of the model\n",
    "res = model.evaluate(test)\n",
    "print res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# record what features are important\n",
    "i_feats = model.get_feature_importance()\n",
    "f_imps = i_feats['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1d31c2da10>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d31cbda50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note that of the 50 features, some have high utility (2600 on the y-axis) and some have low utility (almost 0), but all of them are useful.\n",
    "plt.plot(f_imps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|      feature       | count |\n",
      "+--------------------+-------+\n",
      "|         z1         |  7206 |\n",
      "|         x4         |  6668 |\n",
      "|         z2         |  6566 |\n",
      "|         y2         |  6488 |\n",
      "|         x3         |  6449 |\n",
      "|         y3         |  6436 |\n",
      "|         z3         |  6147 |\n",
      "|         x2         |  5899 |\n",
      "|         z4         |  5703 |\n",
      "|         y1         |  5525 |\n",
      "|         y4         |  5135 |\n",
      "|         x1         |  4574 |\n",
      "|        age         |  1140 |\n",
      "|        user        |  946  |\n",
      "| how_tall_in_meters |  911  |\n",
      "|       weight       |  800  |\n",
      "|  body_mass_index   |  744  |\n",
      "|       gender       |  288  |\n",
      "+--------------------+-------+\n",
      "[18 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect that of the top features we see \n",
    "i_feats.print_rows(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
